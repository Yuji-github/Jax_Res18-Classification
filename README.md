# Jax&Flax ResNet18 Classification Experiment

## Motivation
Constructing deep learning architectures with many layers is expensive as a lot of calculations, such as backpropagation, are needed. One of the solutions is to use Julia, which uses Just-In-Time. However, it does not have a debugging tool because it is not mature enough. Jax and Flax are neural networks for Python, and they use Just-In-Time compilation while training. Thus, Jax and Flax boost training time rapidly. This project reveals Jax and Flax potential with a benchmark dataset in the public domain and points out issues.

## 1.Requirements
- Python ==> 3.8
- Flax
- Jax
- Optax
- Torch
- Numpy

Binary Images:
https://drive.google.com/drive/folders/1RoBdT1k3JI4QMNXOkAx46DxAy-fxMiyO?usp=sharing

## 2.Demo
The video shows the actual execution of Cifer10 with Jax and Flax.

https://user-images.githubusercontent.com/52090852/182778858-e51c2f0c-5f53-4ffb-85e0-b63ed5130408.mp4


## 3.Results
